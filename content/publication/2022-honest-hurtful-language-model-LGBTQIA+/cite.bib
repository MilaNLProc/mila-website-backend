@inproceedings{nozza-etal-2022-measuring,
    title = "Measuring Harmful Sentence Completion in Language Models for {LGBTQIA}+ Individuals",
    author = "Nozza, Debora  and
      Bianchi, Federico  and
      Lauscher, Anne  and
      Hovy, Dirk",
    editor = "Chakravarthi, Bharathi Raja  and
      Bharathi, B  and
      McCrae, John P  and
      Zarrouk, Manel  and
      Bali, Kalika  and
      Buitelaar, Paul",
    booktitle = "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.ltedi-1.4",
    doi = "10.18653/v1/2022.ltedi-1.4",
    pages = "26--34",
    abstract = "Current language technology is ubiquitous and directly influences individuals{'} lives worldwide. Given the recent trend in AI on training and constantly releasing new and powerful large language models (LLMs), there is a need to assess their biases and potential concrete consequences. While some studies have highlighted the shortcomings of these models, there is only little on the negative impact of LLMs on LGBTQIA+ individuals. In this paper, we investigated a state-of-the-art template-based approach for measuring the harmfulness of English LLMs sentence completion when the subjects belong to the LGBTQIA+ community. Our findings show that, on average, the most likely LLM-generated completion is an identity attack 13{\%} of the time. Our results raise serious concerns about the applicability of these models in production environments.",
}

---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation
authors: ["Flor Miriam Plaza-Del-Arco","Debora Nozza","Dirk Hovy"]
date: 2024/5
doi: ''
publishDate: '2025-03-11T14:12:25+01:00'
publication_types:
- '1'
publication: Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives)
  @ LREC-COLING 2024
publication_short: Proceedings of the 3rd Workshop on Perspectivist Approaches to
  NLP (NLPerspectives) @ LREC-COLING 2024
abstract: "Large Language Models (LLMs) exhibit remarkable text classification capabilities,\
  \ excelling in zero- and few-shot learning (ZSL and FSL) scenarios. However, since\
  \ they are trained on different datasets, performance varies widely across tasks\
  \ between those models. Recent studies emphasize the importance of considering human\
  \ label variation in data annotation. However, how this human label variation also\
  \ applies to LLMs remains unexplored. Given this likely model specialization, we\
  \ ask: Do aggregate LLM labels improve over individual models (as for human annotators)?\
  \ We evaluate four recent instruction-tuned LLMs as \u201Cannotators\u201D on five\
  \ subjective tasks across four languages. We use ZSL and FSL setups and label aggregation\
  \ from human annotation. Aggregations are indeed substantially better than any individual\
  \ model, benefiting from specialization in diverse tasks or languages. Surprisingly,\
  \ FSL does not surpass ZSL, as it depends on the quality of the selected examples.\
  \ However, there seems to be no good information-theoretical strategy to select\
  \ those. We find that no LLM method rivals even simple supervised models. We also\
  \ discuss the tradeoffs in accuracy, cost, and moral/ethical considerations between\
  \ LLM and human annotation."
summary: ''
tags:
- annotation
- llms
- nlp
- label variation
categories: []
featured: false
url_pdf: https://aclanthology.org/2024.nlperspectives-1.2.pdf
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
socialmedia_post: Flor Miriam Plaza-del-Arco, {@debora}, {@dirk}'s 2024 paper "Wisdom
  Instruction-Tuned Language Model Crowds" shows that multiple LLMs can be BETTER
  than a single model & "specialize" across tasks & languages. 0-shot outperforms
  few-shot learning, but still can't match supervised models.
image:
  caption: ''
  focal_point: Center
  preview_only: false
projects:
- integrator
slides: ''
# 
---

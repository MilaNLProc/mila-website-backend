---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: 'Comparing Pre-trained Human Language Models: Is it Better with Human Context
  as Groups, Individual Traits, or Both?'
authors: ["Nikita Soni","Niranjan Balasubramanian","H. Andrew Schwartz","Dirk Hovy"]
date: 2024/8
doi: ''
publishDate: '2025-03-11T14:17:46+01:00'
publication_types:
- '1'
publication: Proceedings of the 14th Workshop on Computational Approaches to Subjectivity,
  Sentiment, & Social Media Analysis
publication_short: Proceedings of the 14th Workshop on Computational Approaches to
  Subjectivity, Sentiment, & Social Media Analysis
abstract: "Pre-trained language models consider the context of neighboring words and\
  \ documents but lack any author context of the human generating the text. However,\
  \ language depends on the author\u2019s states, traits, social, situational, and\
  \ environmental attributes, collectively referred to as human context (Soni et al.,\
  \ 2024). Human-centered natural language processing requires incorporating human\
  \ context into language models. Currently, two methods exist: pre-training with\
  \ 1) group-wise attributes (e.g., over-45-year-olds) or 2) individual traits. Group\
  \ attributes are simple but coarse \u2014 not all 45-year-olds write the same way\
  \ \u2014 while individual traits allow for more personalized representations, but\
  \ require more complex modeling and data. It is unclear which approach benefits\
  \ what tasks. We compare pre-training models with human context via 1) group attributes,\
  \ 2) individual users, and 3) a combined approach on five user- and document-level\
  \ tasks. Our results show that there is no best approach, but that human-centered\
  \ language modeling holds avenues for different methods."
summary: ''
tags:
- llms
- nlp
- sociodemographics
categories: []
featured: false
url_pdf: https://aclanthology.org/2024.wassa-1.26.pdf
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
socialmedia_post: "Soni et al.'s (2024) \"Comparing Pre-trained Human Language Models\"\
  \ shows language isn't just words-it's about the PERSON writing them \U0001F464\
  \ However, current LLMs miss crucial human context. One-size-fits-all approaches\
  \ don't work!"
image:
  caption: ''
  focal_point: Center
  preview_only: false
projects: []
slides: ''
# 
---
